{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Loading datasets and libraries"
      ],
      "metadata": {
        "id": "EG8AQJTofVLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from numpy import random"
      ],
      "metadata": {
        "id": "mEdMWFvleRRU"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "MqPFcp4teLFc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Train_set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preproccesing"
      ],
      "metadata": {
        "id": "gYL85XjyfioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple idea of putting 0\n",
        "df.fillna(0, inplace = True)"
      ],
      "metadata": {
        "id": "3edpWyk0fIwy"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the ID and class columns from the input features\n",
        "ids = df['ID']\n",
        "y = df['Class']\n",
        "y = np.array(y)\n",
        "X = df.drop(['ID', 'Class'], axis=1)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "EXQJDU3qfyEV"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)"
      ],
      "metadata": {
        "id": "BDr5zeZu_RSz"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, dimx):\n",
        "        \n",
        "      super().__init__()\n",
        "      self.dimx = dimx\n",
        "      self.Q = nn.Linear(hidden_dim, dimx, bias = False) # 1 for dimension of x - one value\n",
        "      self.K = nn.Linear(hidden_dim, dimx, bias = False)\n",
        "      self.V = nn.Linear(hidden_dim, dimx, bias = False)\n",
        "      self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      queries = self.Q(x[-1,:]).view(-1,1)\n",
        "      keys = self.K(x)\n",
        "      values = torch.transpose(self.V(x),0,1)\n",
        "      preweights = torch.matmul(keys, queries) / np.sqrt(self.dimx)\n",
        "      weights = self.softmax(preweights)\n",
        "      embeddings = torch.matmul(values, weights)\n",
        "      return embeddings"
      ],
      "metadata": {
        "id": "Xcum9Vbsf-iy"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, dimx, output_size, hidden_dim, n_layers = 1,drop=0.5):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dimx = dimx\n",
        "        # Attention\n",
        "        self.attention = Attention(hidden_dim, dimx)\n",
        "        # LSTM layer\n",
        "        self.rnn = nn.LSTM(1, hidden_dim, n_layers)\n",
        "        \n",
        "        # last, fully-connected layer\n",
        "        self.fc1 = nn.Linear(dimx, output_size) \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=0) \n",
        "        \n",
        "        # Capa dropout \n",
        "        self.dropout = nn.Dropout(p=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        - x: Sequences of word embeddings. Dimensions (#no batch, max_l, word_embedding_size)\n",
        "        - lengths: The real length of each sequence, excluding the junk # tokens! You use this to know what\n",
        "          RNN state you should use to classify\n",
        "        '''\n",
        "        #x = x.reshape(x.size(0))\n",
        "        x = np.trim_zeros(x, 'b')\n",
        "        x = torch.Tensor(x).view(-1,1)\n",
        "        x = x.to(torch.float32)\n",
        "        # Compute the RNN output (sequence of states for the whole input)\n",
        "        states, _ =  self.rnn(x)\n",
        "        x = self.attention.forward(states).flatten()\n",
        "        x_drop = self.dropout(x)\n",
        "        x_out = self.fc1(x_drop)\n",
        "        # We classify using such tensor (don't forget the dropout!)\n",
        "        output = self.logsoftmax(x_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-gkdPW2qrHv2"
      },
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_with_train(RNN):\n",
        "    \n",
        "    def __init__(self,dimx, output_size, hidden_dim, n_layers,drop=0.0,batch_size=5000,lr=0.0005,saved_files='./saved_models/RNN_sentiment_analysis'):\n",
        "        \n",
        "        super().__init__(dimx, output_size, hidden_dim, n_layers,drop)  \n",
        "        \n",
        "        self.lr = lr # Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr) # Optimizer\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()              \n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "        self.valid_loss_during_training = [] \n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.saved_files = saved_files\n",
        "            \n",
        "        \n",
        "    def trainloop(self, X_train, X_val, Y, Y_val,epochs=100,print_every=5):\n",
        "        \n",
        "        self.print_every = print_every\n",
        "        self.epochs=epochs\n",
        "        \n",
        "        # Optimization Loop\n",
        "\n",
        "        #labels = torch.Tensor(Y).type(torch.LongTensor) # Training labels\n",
        "        \n",
        "        #labelsval = torch.Tensor(Yval).type(torch.LongTensor) # Validation labels\n",
        "        \n",
        "        ### WE ARE HERE\n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "          for b in range(int(self.batch_size)):\n",
        "            \n",
        "            self.train() # Activate dropout\n",
        "  \n",
        "            # Random data sample, we even out class imbalances during training!\n",
        "            #class_number = random.randint(5) # label\n",
        "            class_number = np.random.choice(5, 1, p=[0.4, 0.15, 0.15, 0.15, 0.15])[0]\n",
        "            idx = random.choice(np.nonzero(Y == class_number)[0]) # choose element to train given a label\n",
        "            \n",
        "            running_loss = 0.\n",
        "            self.optim.zero_grad() \n",
        "            x = X_train[idx,:]\n",
        "            out = self.forward(x)\n",
        "            label = torch.Tensor([class_number]).type(torch.LongTensor)\n",
        "            loss = self.criterion(out.view(1,-1),label)\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "            # SGD steps\n",
        "            self.optim.step()\n",
        "        \n",
        "          self.loss_during_training.append(running_loss/self.batch_size)\n",
        "            \n",
        "        # We save model parameters  \n",
        "          torch.save(self.state_dict(), self.saved_files+'_epoch_'+str(e)+'.pth')\n",
        "            \n",
        "# REAPEAT for valid\n",
        "          with torch.no_grad(): \n",
        "              \n",
        "            # set model to evaluation mode\n",
        "            self.eval()\n",
        "            running_loss = 0.\n",
        "\n",
        "            for b in range(int(self.batch_size)):\n",
        "  \n",
        "              class_number = np.random.choice(5, 1, p=[0.4, 0.15, 0.15, 0.15, 0.15])[0]\n",
        "              idx = random.choice(np.nonzero(Y_val == class_number)[0]) # choose element to train given a label\n",
        "              running_loss = 0.\n",
        "              x = X_val[idx,:]\n",
        "              out = self.forward(x)\n",
        "              label = torch.Tensor([class_number]).type(torch.LongTensor)\n",
        "              loss = self.criterion(out.view(1,-1),label)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "          self.valid_loss_during_training.append(running_loss/self.batch_size)\n",
        "\n",
        "        if(e % self.print_every == 0): \n",
        "\n",
        "          print(f\"Training loss after {e} epochs: {self.loss_during_training[-1]}. Validation loss: {self.valid_loss_during_training[-1]}\")"
      ],
      "metadata": {
        "id": "x-WEsaDQzSHP"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN_with_train(10,5,8,1,0.3, batch_size = 50000) # 50 000 is half the data set\n",
        "rnn.trainloop(X_train, X_val, y_train, y_val,epochs=40) # idk how many epochs"
      ],
      "metadata": {
        "id": "mF7N3No3-53R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rnn.valid_loss_during_training)\n",
        "plt.plot(rnn.loss_during_training, color = \"red\")"
      ],
      "metadata": {
        "id": "bmLlfqkvUqNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('Test_set.csv')\n",
        "df2.fillna(0, inplace = True)\n",
        "# Separate the ID and class columns from the input features\n",
        "ids = df2['ID']\n",
        "X_test = df2.drop(['ID'], axis=1)\n",
        "\n",
        "# Standardize the input features\n",
        "#X_test = scaler.transform(X_test)\n",
        "scaler = StandardScaler()\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "pred_classes = [torch.argmax(rnn.forward(X_test[k,:])) for k in range(X_test.shape[0])]\n",
        "pred_classes = np.array(pred_classes)\n",
        "\n",
        "# Create a new pandas DataFrame with the predicted classes and an ID column (if necessary)\n",
        "ids = np.arange(len(pred_classes))\n",
        "results = pd.DataFrame({'ID': ids, 'Pred_Class': pred_classes})\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "results.to_csv('sub_29-04.csv', index=False)\n",
        "\n",
        "# 0 class\n",
        "sum(pred_classes ==0)"
      ],
      "metadata": {
        "id": "KYm3WRW8Wq6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.head()"
      ],
      "metadata": {
        "id": "umBG0IK7DU_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}