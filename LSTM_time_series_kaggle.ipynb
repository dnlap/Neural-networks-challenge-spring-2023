{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Loading datasets and libraries"
      ],
      "metadata": {
        "id": "EG8AQJTofVLX"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from numpy import random"
      ],
      "metadata": {
        "id": "mEdMWFvleRRU"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "MqPFcp4teLFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c3cb48-e3e1-40f1-d513-bef7f5e72156"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87554, 189)"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ],
      "source": [
        "df = pd.read_csv('Train_set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preproccesing"
      ],
      "metadata": {
        "id": "gYL85XjyfioB"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple idea of putting 0\n",
        "df.fillna(0, inplace = True)"
      ],
      "metadata": {
        "id": "3edpWyk0fIwy"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the ID and class columns from the input features\n",
        "ids = df['ID']\n",
        "y = df['Class']\n",
        "y = np.array(y)\n",
        "X = df.drop(['ID', 'Class'], axis=1)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "EXQJDU3qfyEV"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)"
      ],
      "metadata": {
        "id": "BDr5zeZu_RSz"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, dimx):\n",
        "        \n",
        "      super().__init__()\n",
        "      self.dimx = dimx\n",
        "      self.Q = nn.Linear(1, dimx, bias = False) # 1 for dimension of x - one value\n",
        "      self.K = nn.Linear(1, dimx, bias = False)\n",
        "      self.V = nn.Linear(1, dimx, bias = False)\n",
        "      self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      queries = self.Q(x)\n",
        "      keys = self.K(x)\n",
        "      values = self.V(x)\n",
        "      preweights = torch.matmul(torch.transpose(keys,0,1), queries) / np.sqrt(self.dimx)\n",
        "      weights = self.softmax(preweights)\n",
        "      embeddings = torch.matmul(values, weights)\n",
        "      return embeddings"
      ],
      "metadata": {
        "id": "Xcum9Vbsf-iy"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, dimx, output_size, hidden_dim, n_layers = 1,drop=0.5):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dimx = dimx\n",
        "        # Attention\n",
        "        self.attention = Attention(dimx)\n",
        "        # LSTM layer\n",
        "        self.rnn = nn.LSTM(dimx, hidden_dim, n_layers)\n",
        "        \n",
        "        # last, fully-connected layer\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_size) \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
        "        \n",
        "        # Capa dropout \n",
        "        self.dropout = nn.Dropout(p=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        - x: Sequences of word embeddings. Dimensions (#no batch, max_l, word_embedding_size)\n",
        "        - lengths: The real length of each sequence, excluding the junk # tokens! You use this to know what\n",
        "          RNN state you should use to classify\n",
        "        '''\n",
        "        #x = x.reshape(x.size(0))\n",
        "        x = np.trim_zeros(x, 'b')\n",
        "        x = torch.Tensor(x).view(-1,1)\n",
        "        x = x.to(torch.float32)\n",
        "        x = self.attention.forward(x)\n",
        "        # Compute the RNN output (sequence of states for the whole input)\n",
        "        _, last_state =  self.rnn(x)\n",
        "        last_state = last_state[0]\n",
        "        last_state_drop = self.dropout(last_state)\n",
        "        x_out = self.fc1(last_state_drop)\n",
        "        # We classify using such tensor (don't forget the dropout!)\n",
        "        output = self.logsoftmax(x_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-gkdPW2qrHv2"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_with_train(RNN):\n",
        "    \n",
        "    def __init__(self,dimx, output_size, hidden_dim, n_layers,drop=0.0,batch_size=5000,lr=0.0005,saved_files='./saved_models/RNN_sentiment_analysis'):\n",
        "        \n",
        "        super().__init__(dimx, output_size, hidden_dim, n_layers,drop)  \n",
        "        \n",
        "        self.lr = lr # Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr) # Optimizer\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()               \n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "        self.valid_loss_during_training = [] \n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.saved_files = saved_files\n",
        "            \n",
        "        \n",
        "    def trainloop(self, X_train, X_val, Y, Y_val,epochs=100,print_every=5):\n",
        "        \n",
        "        self.print_every = print_every\n",
        "        self.epochs=epochs\n",
        "        \n",
        "        # Optimization Loop\n",
        "\n",
        "        #labels = torch.Tensor(Y).type(torch.LongTensor) # Training labels\n",
        "        \n",
        "        #labelsval = torch.Tensor(Yval).type(torch.LongTensor) # Validation labels\n",
        "        \n",
        "        ### WE ARE HERE\n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "          for b in range(int(self.batch_size)):\n",
        "            \n",
        "            self.train() # Activate dropout\n",
        "  \n",
        "            # Random data sample, we even out class imbalances during training!\n",
        "            #class_number = random.randint(5) # label\n",
        "            class_number = np.random.choice(5, 1, p=[0.4, 0.15, 0.15, 0.15, 0.15])[0]\n",
        "            idx = random.choice(np.nonzero(Y == class_number)[0]) # choose element to train given a label\n",
        "            \n",
        "            running_loss = 0.\n",
        "            self.optim.zero_grad() \n",
        "            x = X_train[idx,:]\n",
        "            out = self.forward(x)\n",
        "            label = torch.Tensor([class_number]).type(torch.LongTensor)\n",
        "            loss = self.criterion(out,label)\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "            # SGD steps\n",
        "            self.optim.step()\n",
        "        \n",
        "          self.loss_during_training.append(running_loss/self.batch_size)\n",
        "            \n",
        "        # We save model parameters  \n",
        "          torch.save(self.state_dict(), self.saved_files+'_epoch_'+str(e)+'.pth')\n",
        "            \n",
        "# REAPEAT for valid\n",
        "          with torch.no_grad(): \n",
        "              \n",
        "            # set model to evaluation mode\n",
        "            self.eval()\n",
        "            running_loss = 0.\n",
        "\n",
        "            for b in range(int(self.batch_size)):\n",
        "  \n",
        "              class_number = random.randint(5) # label\n",
        "              idx = random.choice(np.nonzero(Y_val == class_number)[0]) # choose element to train given a label\n",
        "              running_loss = 0.\n",
        "              x = X_val[idx,:]\n",
        "              out = self.forward(x)\n",
        "              label = torch.Tensor([class_number]).type(torch.LongTensor)\n",
        "              loss = self.criterion(out,label)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "          self.valid_loss_during_training.append(running_loss/self.batch_size)\n",
        "\n",
        "        if(e % self.print_every == 0): \n",
        "\n",
        "          print(f\"Training loss after {e} epochs: {self.loss_during_training[-1]}. Validation loss: {self.valid_loss_during_training[-1]}\")"
      ],
      "metadata": {
        "id": "x-WEsaDQzSHP"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN_with_train(10,5,8,1,0.3, batch_size = 50000) # 50 000 is half the data set\n",
        "rnn.trainloop(X_train, X_val, y_train, y_val,epochs=30) # idk how many epochs"
      ],
      "metadata": {
        "id": "mF7N3No3-53R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "83432804-2c14-4600-f9d8-2f85497b8f97"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'prod'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-330-7e6a3481177f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_with_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 50 000 is half the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# idk how many epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-329-fe0afa7b35b5>\u001b[0m in \u001b[0;36mtrainloop\u001b[0;34m(self, X_train, X_val, Y, Y_val, epochs, print_every)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Random data sample, we even out class imbalances during training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#class_number = random.randint(5) # label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mclass_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclass_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# choose element to train given a label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3086\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m     \"\"\"\n\u001b[0;32m-> 3088\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3089\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rnn.valid_loss_during_training)\n",
        "plt.plot(rnn.loss_during_training, color = \"red\")"
      ],
      "metadata": {
        "id": "bmLlfqkvUqNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('Test_set.csv')\n",
        "df2.fillna(0, inplace = True)\n",
        "# Separate the ID and class columns from the input features\n",
        "ids = df2['ID']\n",
        "X_test = df2.drop(['ID'], axis=1)\n",
        "\n",
        "# Standardize the input features\n",
        "#X_test = scaler.transform(X_test)\n",
        "scaler = StandardScaler()\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "pred_classes = [torch.argmax(rnn.forward(X_test[k,:])) for k in range(X_test.shape[0])]\n",
        "pred_classes = np.array(pred_classes)\n",
        "\n",
        "# Create a new pandas DataFrame with the predicted classes and an ID column (if necessary)\n",
        "ids = np.arange(len(pred_classes))\n",
        "df = pd.DataFrame({'ID': ids, 'Pred_Class': pred_classes})\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df.to_csv('sub_29-04.csv', index=False)\n",
        "\n",
        "# 0 class\n",
        "sum(pred_classes ==0)"
      ],
      "metadata": {
        "id": "KYm3WRW8Wq6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}